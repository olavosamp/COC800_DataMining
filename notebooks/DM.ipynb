{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy                as np\n",
    "import pandas               as pd\n",
    "# import sklearn              as sk\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import dirs\n",
    "from load_dataset           import load_dataset\n",
    "from preproc                import preproc, dimension_reduction\n",
    "\n",
    "from bayes                  import gaussian_naive_bayes\n",
    "from show_class_splits      import show_class_splits\n",
    "from least_squares          import least_squares, ridge_least_squares\n",
    "from logistic_regression    import log_reg, ridge_log_reg\n",
    "from perceptron             import perceptron\n",
    "from nearest_neighbours     import nearest_neighbours\n",
    "from decision_trees         import decision_tree, random_forest, ada_boost\n",
    "from discriminant_analysis  import linear_discriminant_analysis, quadratic_discriminant_analysis\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25592\n",
      "\n",
      "\n",
      "---- Loading and Preprocessing ----\n",
      "('\\nclassPos: ', 63981)\n",
      "('classNeg: ', 1245005)\n",
      "\n",
      "\n",
      "Data loaded with following class distribution: \n",
      "Positive class: 0.00 %, 63981 entries \n",
      "Negative class: 0.00 %, 63981 entries \n",
      "Total:          127962 entries\n",
      "\n",
      "3 features containing only zeros have been dropped from data.\n",
      "\n",
      "Train data loaded with following class distributions:\n",
      "Positive class: 0.00 %, 51013 entries \n",
      "Negative class: 0.00 %, 51357 entries \n",
      "Total:          102370 entries\n",
      "\n",
      "Test data loaded with following class distributions:\n",
      "Positive class: 0.00 %, 12968 entries \n",
      "Negative class: 0.00 %, 12624 entries \n",
      "Total:          25592 entries\n",
      "('\\nN components:', 97)\n",
      "('\\nPrincipal components to keep: ', 60)\n",
      "('\\nCompact data: ', (127962, 60))\n",
      "\n",
      "\n",
      "---- Classification ----\n",
      "\n",
      "\n",
      "LDA\n",
      "Elapsed: 0:00:00.610336s\n",
      "Correct predictions 0.9271\n",
      "\n",
      "QDA\n",
      "Elapsed: 0:00:01.060749s\n",
      "Correct predictions 0.9448\n",
      "\n",
      "Naive Bayes\n",
      "Elapsed: 0:00:00.086787s\n",
      "Correct predictions 0.9438\n",
      "\n",
      "Logistic Regression\n",
      "Elapsed: 0:00:02.863754s\n",
      "Correct predictions 0.9696\n",
      "\n",
      "Linear Perceptron\n",
      "Elapsed: 0:00:00.186024s\n",
      "Correct predictions 0.9696\n",
      "\n",
      "Decision Tree\n",
      "Elapsed: 0:00:21.664850s\n",
      "Correct predictions 0.9497\n",
      "\n",
      "Random Forest\n",
      "Elapsed: 0:00:10.912913s\n",
      "Correct predictions 0.9664\n",
      "\n",
      "AdaBoost\n",
      "Elapsed: 0:00:58.480582s\n",
      "Correct predictions 0.9654\n"
     ]
    }
   ],
   "source": [
    "# np.set_printoptions(precision=4)\n",
    "\n",
    "numPos   = 63981   # Max of    63 981 samples\n",
    "numNeg   = 63981   # Max of 1 245 005 samples\n",
    "\n",
    "#numPos   = 20000   # Max of    63 981 samples\n",
    "#numNeg   = 20000   # Max of 1 245 005 samples\n",
    "testSize = int((numPos+numNeg)*0.2)\n",
    "print(testSize)\n",
    "print(\"\\n\\n---- Loading and Preprocessing ----\")\n",
    "\n",
    "dataDf, labels = load_dataset(dirs.dataset, randomState=None, numPos=numPos, numNeg=numNeg)#fracPos=0.02, fracNeg=0.02)\n",
    "dataDf = preproc(dataDf, verbose=False)\n",
    "# labeledDf = dataDf.assign(Labels=labels)\n",
    "\n",
    "trainDf, testDf, y_train, y_test = train_test_split(dataDf, labels, test_size=testSize)\n",
    "\n",
    "print(\"\\nTrain data loaded with following class distributions:\")\n",
    "show_class_splits(y_train)\n",
    "print(\"\\nTest data loaded with following class distributions:\")\n",
    "show_class_splits(y_test)\n",
    "\n",
    "'Principal Components Analysis'\n",
    "'   useful to reduce dataset dimensionality'\n",
    "compactDf = dimension_reduction(dataDf, keepComp=60)\n",
    "\n",
    "print(\"\\n\\n---- Classification ----\\n\")\n",
    "\n",
    "\n",
    "\n",
    "'Linear Discriminant Classifier'\n",
    "print(\"\\nLDA\")\n",
    "startTime = datetime.now()\n",
    "ldaPred = linear_discriminant_analysis(trainDf, y_train, testDf, y_test)\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "print(\"Correct predictions {:.4f}\".format(float(np.sum(ldaPred == y_test))/testSize))\n",
    "\n",
    "'Quadratic Discriminant Classifier'\n",
    "print(\"\\nQDA\")\n",
    "startTime = datetime.now()\n",
    "qdaPred = quadratic_discriminant_analysis(trainDf, y_train, testDf, y_test)\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "print(\"Correct predictions {:.4f}\".format(float(np.sum(qdaPred == y_test))/testSize))\n",
    "\n",
    "'Bayesian Classifier'\n",
    "print(\"\\nNaive Bayes\")\n",
    "startTime = datetime.now()\n",
    "bayesPred = gaussian_naive_bayes(trainDf, y_train, testDf, y_test)\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "print(\"Correct predictions {:.4f}\".format(float(np.sum(bayesPred == y_test))/testSize))\n",
    "\n",
    "'Logistic Regression'\n",
    "print(\"\\nLogistic Regression\")\n",
    "startTime = datetime.now()\n",
    "logPred = log_reg(trainDf, y_train, testDf, y_test)\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "print(\"Correct predictions {:.4f}\".format(float(np.sum(logPred == y_test))/testSize))\n",
    "\n",
    "#@@'Logistic Regression with L2 Regularization'\n",
    "#@@# TODO: Testar LogisticRegressionCV, que encontra o C otimo\n",
    "#@@logPenalty = 1/100\n",
    "\n",
    "#@@print(\"\\nLogistic Regression with L2 Regularization\")\n",
    "#@@startTime = datetime.now()\n",
    "#@@rlogPred = ridge_log_reg(trainDf, y_train, testDf, y_test, reg=logPenalty)\n",
    "#@@elapsed = datetime.now() - startTime\n",
    "#@@print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#@@print(\"Regularization paramenter (smaller is stronger): \\n\", logPenalty)\n",
    "#@@print(\"Correct predictions {:.4f}\".format(np.sum(rlogPred == y_test)/testSize))\n",
    "\n",
    "'Linear Perceptron'\n",
    "print(\"\\nLinear Perceptron\")\n",
    "startTime = datetime.now()\n",
    "percepPred = perceptron(trainDf, y_train, testDf, y_test)\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "print(\"Correct predictions {:.4f}\".format(float(np.sum(logPred == y_test))/testSize))\n",
    "\n",
    "# 'Nearest Neighbours'\n",
    "# start = time.perf_counter()\n",
    "# knnPred = nearest_neighbours(trainDf, y_train, testDf, y_test)\n",
    "# elapsed = time.perf_counter() - start\n",
    "# print(\"\\nNearest Neighbours\")\n",
    "# print(\"Elapsed: {:.2f}s\".format(elapsed))\n",
    "# print(\"Correct predictions {:.4f}\".format(np.sum(knnPred == y_test)/testSize))\n",
    "\n",
    "'Decision Tree'\n",
    "print(\"\\nDecision Tree\")\n",
    "startTime = datetime.now()\n",
    "treePred = decision_tree(trainDf, y_train, testDf, y_test)\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "print(\"Correct predictions {:.4f}\".format(float(np.sum(treePred == y_test))/testSize))\n",
    "\n",
    "'Random Forest'\n",
    "print(\"\\nRandom Forest\")\n",
    "startTime = datetime.now()\n",
    "forestPred = random_forest(trainDf, y_train, testDf, y_test)\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "print(\"Correct predictions {:.4f}\".format(float(np.sum(forestPred == y_test))/testSize))\n",
    "\n",
    "'AdaBoost'\n",
    "print(\"\\nAdaBoost\")\n",
    "startTime = datetime.now()\n",
    "forestPred = ada_boost(trainDf, y_train, testDf, y_test)\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "print(\"Correct predictions {:.4f}\".format(float(np.sum(forestPred == y_test))/testSize))\n",
    "\n",
    "\n",
    "# Ensembles\n",
    "#   Bagging (Pasting algorithm)\n",
    "#   Random Forest\n",
    "#   Ada/Gradient Boost ou similares\n",
    "\n",
    "#   Regressão Polinomial (criação de novas features)\n",
    "\n",
    "#\n",
    "# Discriminador Linear      (LDA)\n",
    "# Discriminador Quadrático  (QDA)\n",
    "#\n",
    "# Rede neural MLP\n",
    "#\n",
    "# SVM Linear\n",
    "# SVM Outros kernels\n",
    "#\n",
    "# Mistura Gaussiana (Não supervisionado)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fit_time',\n",
       " 'score_time',\n",
       " 'test_prec_macro',\n",
       " 'test_rec_micro',\n",
       " 'train_prec_macro',\n",
       " 'train_rec_micro']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.data.shape, iris.target.shape\n",
    "((150, 4), (150,))\n",
    "\n",
    "\n",
    "'Linear Discriminant Classifier'\n",
    "print(\"\\nLDA\")\n",
    "#startTime = datetime.now()\n",
    "ldaPred = linear_discriminant_analysis(trainDf, y_train, testDf, y_test)\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "print(\"Correct predictions {:.4f}\".format(float(np.sum(ldaPred == y_test))/testSize))\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "#scores = cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "#scores.shape                                              \n",
    "\n",
    "\n",
    "scoring = {'prec_macro': 'precision_macro','rec_micro': make_scorer(recall_score, average='macro')}\n",
    "scores = cross_validate(clf, iris.data, iris.target, scoring=scoring,cv=5, return_train_score=True)\n",
    "sorted(scores.keys())                 \n",
    "#scores['train_rec_micro']                         \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
