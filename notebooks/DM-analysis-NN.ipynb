{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TabError",
     "evalue": "inconsistent use of tabs and spaces in indentation (cross_val_analysis.py, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"E:\\Program Files\\Arquivos Incomuns\\UFRJ\\Data Mining COC800\\Trabalho\\COC800_DataMining\\cross_val_analysis.py\"\u001b[1;36m, line \u001b[1;32m50\u001b[0m\n\u001b[1;33m    pred_ = model.predict(x.iloc[val])\u001b[0m\n\u001b[1;37m                                      ^\u001b[0m\n\u001b[1;31mTabError\u001b[0m\u001b[1;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
     ]
    }
   ],
   "source": [
    "import numpy                as np\n",
    "import pandas               as pd\n",
    "# import sklearn              as sk\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import dirs\n",
    "from load_dataset           import load_dataset\n",
    "from preproc                import preproc, dimension_reduction\n",
    "\n",
    "from bayes                  import gaussian_naive_bayes\n",
    "from show_class_splits      import show_class_splits\n",
    "from least_squares          import least_squares, ridge_least_squares\n",
    "from logistic_regression    import log_reg, ridge_log_reg\n",
    "from perceptron             import perceptron\n",
    "from nearest_neighbours     import nearest_neighbours\n",
    "from decision_trees         import decision_tree, random_forest, ada_boost\n",
    "from discriminant_analysis  import linear_discriminant_analysis, quadratic_discriminant_analysis\n",
    "from cross_val_analysis     import cross_val_analysis\n",
    "from cross_val_analysis     import cross_val_analysis_nn\n",
    "from vis_functions          import format_as_table\n",
    "from neural_networks import multi_layer_perceptron\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score,precision_score,recall_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "# np.set_printoptions(precision=4)\n",
    "\n",
    "#numPos   = 63981   # Max of    63 981 samples\n",
    "#numNeg   = 63981   # Max of 1 245 005 samples\n",
    "\n",
    "# numPos   = 63981   # Max of    63 981 samples\n",
    "# numNeg   = 1245005   # Max of 1 245 005 samples\n",
    "\n",
    "#numPos   = 20000   # Max of    63 981 samples\n",
    "#numNeg   = 20000   # Max of 1 245 005 samples\n",
    "\n",
    "\n",
    "print(\"\\n\\n---- Loading and Preprocessing ----\")\n",
    "\n",
    "dataDf, labels = load_dataset(dirs.dataset, randomState=None, fracPos=0.02, fracNeg=0.02)#numPos=numPos, numNeg=numNeg)#\n",
    "\n",
    "testSize = round(dataDf.shape[0]*0.2)\n",
    "dataDf = preproc(dataDf, verbose=False)\n",
    "# labeledDf = dataDf.assign(Labels=labels)\n",
    "\n",
    "trainDf, testDf, y_train, y_test = train_test_split(dataDf, labels, test_size=testSize)\n",
    "\n",
    "print(\"\\nTrain data loaded with following class distributions:\")\n",
    "show_class_splits(y_train)\n",
    "print(\"\\nTest data loaded with following class distributions:\")\n",
    "show_class_splits(y_test)\n",
    "\n",
    "'Principal Components Analysis'\n",
    "'   useful to reduce dataset dimensionality'\n",
    "compactDf = dimension_reduction(dataDf, keepComp=60)\n",
    "\n",
    "print(\"\\n\\n---- Classification ----\\n\")\n",
    "\n",
    "metrics_=[]\n",
    "\n",
    "'Neural Networks'\n",
    "print(\"\\nMLP\")\n",
    "startTime = datetime.now()\n",
    "mlp = multi_layer_perceptron(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis_nn(n_split=2,classifier=mlp,x=trainDf, y=y_train,model_name=\"MLP\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(ldaPred == y_test))/testSize))\n",
    "#print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, ldaPred, normalize=True)))\n",
    "#print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, ldaPred)))\n",
    "#print(\"AUC: {:.2f}\".format(100*roc_auc_score(y_test, ldaPred)))\n",
    "#print(\"Precision: {:.2f}\".format(100*precision_score(y_test, ldaPred)))\n",
    "#print(\"Recall: {:.2f}\".format(100*recall_score(y_test, ldaPred)))\n",
    "#target_names = ['Signal', 'Background']\n",
    "#print(classification_report(y_test, ldaPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "#sns.heatmap(confusion_matrix(y_test, ldaPred),annot=True, cmap = 'YlGnBu')\n",
    "#plt.title(\"Confusion Matrix\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(995849, 51340)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf[y_train==-1].shape[0],trainDf[y_train==1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261797\n",
      "\n",
      "\n",
      "---- Loading and Preprocessing ----\n",
      "\n",
      "Original data:\n",
      "Positive examples:  63981\n",
      "Negative examples: 1245005\n",
      "\n",
      "\n",
      "Data loaded with following class distribution: \n",
      "Positive class:  0.00 %, 63981 entries \n",
      "Negative class:  0.00 %, 1245005 entries \n",
      "Total:          1308986 entries\n",
      "\n",
      "3 features containing only zeros have been dropped from data.\n",
      "\n",
      "Train data loaded with following class distributions:\n",
      "Positive class:  0.00 %, 51340 entries \n",
      "Negative class:  0.00 %, 995849 entries \n",
      "Total:          1047189 entries\n",
      "\n",
      "Test data loaded with following class distributions:\n",
      "Positive class:  0.00 %, 12641 entries \n",
      "Negative class:  0.00 %, 249156 entries \n",
      "Total:          261797 entries\n",
      "('\\nN components:', 97)\n",
      "('\\nPrincipal components to keep: ', 60)\n",
      "('\\nCompact data: ', (1308986, 60))\n",
      "\n",
      "\n",
      "---- Classification ----\n",
      "\n",
      "\n",
      "LDA\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1131e33b561a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLDA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mstartTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mldaPred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_discriminant_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mmetrics_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"LDA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caducovas/DeepRinger/COC800_DataMining/discriminant_analysis.pyc\u001b[0m in \u001b[0;36mlinear_discriminant_analysis\u001b[0;34m(x_train, y_train, x_test, y_test, n_components)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#X_r2 = lda.fit(x_train, y_train).transform(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/sklearn/discriminant_analysis.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshrinkage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shrinkage not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve_svd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lsqr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve_lsqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshrinkage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshrinkage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/sklearn/discriminant_analysis.pyc\u001b[0m in \u001b[0;36m_solve_svd\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfac\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mXc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# SVD of centered (within)scaled data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/scipy/linalg/decomp_svd.pyc\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# perform decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0;32m--> 129\u001b[0;31m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "# np.set_printoptions(precision=4)\n",
    "\n",
    "#numPos   = 63981   # Max of    63 981 samples\n",
    "#numNeg   = 63981   # Max of 1 245 005 samples\n",
    "\n",
    "numPos   = 63981   # Max of    63 981 samples\n",
    "numNeg   = 1245005   # Max of 1 245 005 samples\n",
    "\n",
    "#numPos   = 20000   # Max of    63 981 samples\n",
    "#numNeg   = 20000   # Max of 1 245 005 samples\n",
    "testSize = int((numPos+numNeg)*0.2)\n",
    "print(testSize)\n",
    "print(\"\\n\\n---- Loading and Preprocessing ----\")\n",
    "\n",
    "dataDf, labels = load_dataset(dirs.dataset, randomState=None, numPos=numPos, numNeg=numNeg)#fracPos=0.02, fracNeg=0.02)\n",
    "dataDf = preproc(dataDf, verbose=False)\n",
    "# labeledDf = dataDf.assign(Labels=labels)\n",
    "\n",
    "trainDf, testDf, y_train, y_test = train_test_split(dataDf, labels, test_size=testSize)\n",
    "\n",
    "print(\"\\nTrain data loaded with following class distributions:\")\n",
    "show_class_splits(y_train)\n",
    "print(\"\\nTest data loaded with following class distributions:\")\n",
    "show_class_splits(y_test)\n",
    "\n",
    "'Principal Components Analysis'\n",
    "'   useful to reduce dataset dimensionality'\n",
    "compactDf = dimension_reduction(dataDf, keepComp=60)\n",
    "\n",
    "print(\"\\n\\n---- Classification ----\\n\")\n",
    "\n",
    "metrics_=[]\n",
    "\n",
    "'Linear Discriminant Classifier'\n",
    "print(\"\\nLDA\")\n",
    "startTime = datetime.now()\n",
    "ldaPred,lda = linear_discriminant_analysis(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=lda,x=trainDf, y=y_train,model_name=\"LDA\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(ldaPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, ldaPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, ldaPred)))\n",
    "print(\"AUC: {:.2f}\".format(100*roc_auc_score(y_test, ldaPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, ldaPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, ldaPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, ldaPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, ldaPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Quadratic Discriminant Classifier'\n",
    "print(\"\\nQDA\")\n",
    "startTime = datetime.now()\n",
    "qdaPred,qda = quadratic_discriminant_analysis(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=qda,x=trainDf, y=y_train,model_name=\"QDA\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(qdaPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, qdaPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, qdaPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, qdaPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, qdaPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, qdaPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, qdaPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, qdaPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Bayesian Classifier'\n",
    "print(\"\\nNaive Bayes\")\n",
    "startTime = datetime.now()\n",
    "bayesPred,bayes = gaussian_naive_bayes(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=bayes,x=trainDf, y=y_train,model_name=\"Bayesian Classifier\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(bayesPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, bayesPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, bayesPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, bayesPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, bayesPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, bayesPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, bayesPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, bayesPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Logistic Regression'\n",
    "print(\"\\nLogistic Regression\")\n",
    "startTime = datetime.now()\n",
    "logPred,log = log_reg(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=log,x=trainDf, y=y_train,model_name=\"Logistic Regression\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(logPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, logPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, logPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, logPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, logPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, logPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, logPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, logPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "#@@'Logistic Regression with L2 Regularization'\n",
    "#@@# TODO: Testar LogisticRegressionCV, que encontra o C otimo\n",
    "#@@logPenalty = 1/100\n",
    "\n",
    "#@@print(\"\\nLogistic Regression with L2 Regularization\")\n",
    "#@@startTime = datetime.now()\n",
    "#@@rlogPred = ridge_log_reg(trainDf, y_train, testDf, y_test, reg=logPenalty)\n",
    "#@@elapsed = datetime.now() - startTime\n",
    "#@@print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#@@print(\"Regularization paramenter (smaller is stronger): \\n\", logPenalty)\n",
    "#@@print(\"Correct predictions {:.4f}\".format(np.sum(rlogPred == y_test)/testSize))\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Linear Perceptron'\n",
    "print(\"\\nLinear Perceptron\")\n",
    "startTime = datetime.now()\n",
    "percepPred,percep = perceptron(trainDf, y_train, testDf, y_test)\n",
    "#cross_val_analysis(n_split=2,classifier=percep,trainDf=trainDf, y_train=y_train)\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(logPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, percepPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, percepPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, percepPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, percepPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, percepPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, percepPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, percepPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Nearest Neighbours'\n",
    "startTime = datetime.now()\n",
    "knnPred,knn = nearest_neighbours(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=knn,x=trainDf, y=y_train,model_name=\"Nearest Neighbours\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "print(\"\\nNearest Neighbours\")\n",
    "#print(\"Correct predictions {:.4f}\".format(np.sum(knnPred == y_test)/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, knnPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, knnPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, knnPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, knnPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, knnPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, knnPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, knnPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Decision Tree'\n",
    "print(\"\\nDecision Tree\")\n",
    "startTime = datetime.now()\n",
    "treePred,tree = decision_tree(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=tree,x=trainDf, y=y_train,model_name=\"Decision Tree\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(treePred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, treePred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, treePred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, treePred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, treePred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, treePred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, treePred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, treePred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Random Forest'\n",
    "print(\"\\nRandom Forest\")\n",
    "startTime = datetime.now()\n",
    "forestPred,forest = random_forest(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=forest,x=trainDf, y=y_train,model_name=\"Random Forest\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(forestPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, forestPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, forestPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, forestPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, forestPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, forestPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, forestPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, forestPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'AdaBoost'\n",
    "print(\"\\nAdaBoost\")\n",
    "startTime = datetime.now()\n",
    "adaPred,ada = ada_boost(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=ada,x=trainDf, y=y_train,model_name=\"AdaBoost\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(adaPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, adaPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, adaPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, adaPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, adaPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, adaPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, adaPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, adaPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "header = ['Model', 'Accuracy', 'Std','F1-Measure','Std','Precision','Std','Fecall','Std','ROC AUC','Std']\n",
    "keys = ['model', 'accuracy', 'accuracy_std','f1','f1_std','precision','precision_std','recall','recall_std','roc_auc','roc_auc_std']\n",
    "#sort_by_key = 'age'\n",
    "#sort_order_reverse = True\n",
    "#del(formatted_data)\n",
    "print(format_as_table(metrics_,keys,header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
