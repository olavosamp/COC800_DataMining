{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy                as np\n",
    "import pandas               as pd\n",
    "# import sklearn              as sk\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import dirs\n",
    "from load_dataset           import load_dataset\n",
    "from preproc                import preproc, dimension_reduction\n",
    "\n",
    "from bayes                  import gaussian_naive_bayes\n",
    "from show_class_splits      import show_class_splits\n",
    "from least_squares          import least_squares, ridge_least_squares\n",
    "from logistic_regression    import log_reg, ridge_log_reg\n",
    "from perceptron             import perceptron\n",
    "from nearest_neighbours     import nearest_neighbours\n",
    "from decision_trees         import decision_tree, random_forest, ada_boost\n",
    "from discriminant_analysis  import linear_discriminant_analysis, quadratic_discriminant_analysis\n",
    "from cross_val_analysis     import cross_val_analysis\n",
    "from cross_val_analysis     import cross_val_analysis_nn\n",
    "from vis_functions          import format_as_table\n",
    "from neural_networks import multi_layer_perceptron\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score,precision_score,recall_score,classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "import keras.callbacks as callbacks\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261797\n",
      "\n",
      "\n",
      "---- Loading and Preprocessing ----\n",
      "\n",
      "Original data:\n",
      "Positive examples:  63981\n",
      "Negative examples: 1245005\n",
      "\n",
      "\n",
      "Data loaded with following class distribution: \n",
      "Positive class:  0.00 %, 63981 entries \n",
      "Negative class:  0.00 %, 1245005 entries \n",
      "Total:          1308986 entries\n",
      "\n",
      "3 features containing only zeros have been dropped from data.\n",
      "\n",
      "Train data loaded with following class distributions:\n",
      "Positive class:  0.00 %, 51144 entries \n",
      "Negative class:  0.00 %, 996045 entries \n",
      "Total:          1047189 entries\n",
      "\n",
      "Test data loaded with following class distributions:\n",
      "Positive class:  0.00 %, 12837 entries \n",
      "Negative class:  0.00 %, 248960 entries \n",
      "Total:          261797 entries\n",
      "('\\nN components:', 97)\n",
      "('\\nPrincipal components to keep: ', 60)\n",
      "('\\nCompact data: ', (1308986, 60))\n",
      "\n",
      "\n",
      "---- Classification ----\n",
      "\n",
      "\n",
      "MLP\n",
      "Train Process for 1 Fold\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/keras/models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 523594 samples, validate on 523595 samples\n",
      "Epoch 1/500\n",
      " - 53s - loss: 0.0377 - mean_absolute_error: 0.0477 - mean_absolute_percentage_error: 4.7657 - acc: 0.9749 - categorical_accuracy: 1.0000 - val_loss: 0.0257 - val_mean_absolute_error: 0.0258 - val_mean_absolute_percentage_error: 2.5816 - val_acc: 0.9866 - val_categorical_accuracy: 1.0000\n",
      "Epoch 2/500\n",
      " - 51s - loss: 0.0243 - mean_absolute_error: 0.0245 - mean_absolute_percentage_error: 2.4518 - acc: 0.9871 - categorical_accuracy: 1.0000 - val_loss: 0.0244 - val_mean_absolute_error: 0.0234 - val_mean_absolute_percentage_error: 2.3447 - val_acc: 0.9875 - val_categorical_accuracy: 1.0000\n",
      "Epoch 3/500\n",
      " - 53s - loss: 0.0233 - mean_absolute_error: 0.0225 - mean_absolute_percentage_error: 2.2463 - acc: 0.9879 - categorical_accuracy: 1.0000 - val_loss: 0.0238 - val_mean_absolute_error: 0.0226 - val_mean_absolute_percentage_error: 2.2558 - val_acc: 0.9879 - val_categorical_accuracy: 1.0000\n",
      "Epoch 4/500\n",
      " - 51s - loss: 0.0229 - mean_absolute_error: 0.0217 - mean_absolute_percentage_error: 2.1719 - acc: 0.9883 - categorical_accuracy: 1.0000 - val_loss: 0.0244 - val_mean_absolute_error: 0.0227 - val_mean_absolute_percentage_error: 2.2711 - val_acc: 0.9877 - val_categorical_accuracy: 1.0000\n",
      "Epoch 5/500\n",
      " - 50s - loss: 0.0226 - mean_absolute_error: 0.0211 - mean_absolute_percentage_error: 2.1059 - acc: 0.9885 - categorical_accuracy: 1.0000 - val_loss: 0.0243 - val_mean_absolute_error: 0.0215 - val_mean_absolute_percentage_error: 2.1524 - val_acc: 0.9881 - val_categorical_accuracy: 1.0000\n",
      "Epoch 6/500\n",
      " - 52s - loss: 0.0224 - mean_absolute_error: 0.0206 - mean_absolute_percentage_error: 2.0572 - acc: 0.9886 - categorical_accuracy: 1.0000 - val_loss: 0.0244 - val_mean_absolute_error: 0.0214 - val_mean_absolute_percentage_error: 2.1449 - val_acc: 0.9880 - val_categorical_accuracy: 1.0000\n",
      "Epoch 7/500\n",
      " - 52s - loss: 0.0222 - mean_absolute_error: 0.0203 - mean_absolute_percentage_error: 2.0265 - acc: 0.9888 - categorical_accuracy: 1.0000 - val_loss: 0.0245 - val_mean_absolute_error: 0.0216 - val_mean_absolute_percentage_error: 2.1639 - val_acc: 0.9880 - val_categorical_accuracy: 1.0000\n",
      "Epoch 8/500\n",
      " - 52s - loss: 0.0219 - mean_absolute_error: 0.0198 - mean_absolute_percentage_error: 1.9830 - acc: 0.9890 - categorical_accuracy: 1.0000 - val_loss: 0.0241 - val_mean_absolute_error: 0.0202 - val_mean_absolute_percentage_error: 2.0195 - val_acc: 0.9887 - val_categorical_accuracy: 1.0000\n",
      "Epoch 9/500\n",
      " - 53s - loss: 0.0218 - mean_absolute_error: 0.0194 - mean_absolute_percentage_error: 1.9413 - acc: 0.9891 - categorical_accuracy: 1.0000 - val_loss: 0.0245 - val_mean_absolute_error: 0.0209 - val_mean_absolute_percentage_error: 2.0884 - val_acc: 0.9883 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/500\n",
      " - 49s - loss: 0.0216 - mean_absolute_error: 0.0192 - mean_absolute_percentage_error: 1.9242 - acc: 0.9892 - categorical_accuracy: 1.0000 - val_loss: 0.0246 - val_mean_absolute_error: 0.0207 - val_mean_absolute_percentage_error: 2.0673 - val_acc: 0.9882 - val_categorical_accuracy: 1.0000\n",
      "Epoch 11/500\n",
      " - 51s - loss: 0.0216 - mean_absolute_error: 0.0192 - mean_absolute_percentage_error: 1.9153 - acc: 0.9892 - categorical_accuracy: 1.0000 - val_loss: 0.0242 - val_mean_absolute_error: 0.0207 - val_mean_absolute_percentage_error: 2.0690 - val_acc: 0.9884 - val_categorical_accuracy: 1.0000\n",
      "Epoch 12/500\n",
      " - 49s - loss: 0.0214 - mean_absolute_error: 0.0192 - mean_absolute_percentage_error: 1.9168 - acc: 0.9894 - categorical_accuracy: 1.0000 - val_loss: 0.0247 - val_mean_absolute_error: 0.0204 - val_mean_absolute_percentage_error: 2.0350 - val_acc: 0.9883 - val_categorical_accuracy: 1.0000\n",
      "Epoch 13/500\n",
      " - 52s - loss: 0.0213 - mean_absolute_error: 0.0188 - mean_absolute_percentage_error: 1.8777 - acc: 0.9894 - categorical_accuracy: 1.0000 - val_loss: 0.0245 - val_mean_absolute_error: 0.0199 - val_mean_absolute_percentage_error: 1.9896 - val_acc: 0.9887 - val_categorical_accuracy: 1.0000\n",
      "Epoch 14/500\n",
      " - 50s - loss: 0.0213 - mean_absolute_error: 0.0185 - mean_absolute_percentage_error: 1.8486 - acc: 0.9895 - categorical_accuracy: 1.0000 - val_loss: 0.0248 - val_mean_absolute_error: 0.0201 - val_mean_absolute_percentage_error: 2.0055 - val_acc: 0.9886 - val_categorical_accuracy: 1.0000\n",
      "Epoch 15/500\n",
      " - 53s - loss: 0.0212 - mean_absolute_error: 0.0182 - mean_absolute_percentage_error: 1.8187 - acc: 0.9896 - categorical_accuracy: 1.0000 - val_loss: 0.0246 - val_mean_absolute_error: 0.0198 - val_mean_absolute_percentage_error: 1.9803 - val_acc: 0.9886 - val_categorical_accuracy: 1.0000\n",
      "Epoch 16/500\n",
      " - 53s - loss: 0.0211 - mean_absolute_error: 0.0181 - mean_absolute_percentage_error: 1.8134 - acc: 0.9897 - categorical_accuracy: 1.0000 - val_loss: 0.0245 - val_mean_absolute_error: 0.0200 - val_mean_absolute_percentage_error: 2.0003 - val_acc: 0.9885 - val_categorical_accuracy: 1.0000\n",
      "Epoch 17/500\n",
      " - 50s - loss: 0.0209 - mean_absolute_error: 0.0179 - mean_absolute_percentage_error: 1.7928 - acc: 0.9898 - categorical_accuracy: 1.0000 - val_loss: 0.0249 - val_mean_absolute_error: 0.0200 - val_mean_absolute_percentage_error: 2.0041 - val_acc: 0.9884 - val_categorical_accuracy: 1.0000\n",
      "Epoch 18/500\n",
      " - 50s - loss: 0.0209 - mean_absolute_error: 0.0178 - mean_absolute_percentage_error: 1.7781 - acc: 0.9898 - categorical_accuracy: 1.0000 - val_loss: 0.0250 - val_mean_absolute_error: 0.0201 - val_mean_absolute_percentage_error: 2.0062 - val_acc: 0.9883 - val_categorical_accuracy: 1.0000\n",
      "Epoch 19/500\n",
      " - 51s - loss: 0.0209 - mean_absolute_error: 0.0178 - mean_absolute_percentage_error: 1.7832 - acc: 0.9898 - categorical_accuracy: 1.0000 - val_loss: 0.0250 - val_mean_absolute_error: 0.0201 - val_mean_absolute_percentage_error: 2.0066 - val_acc: 0.9884 - val_categorical_accuracy: 1.0000\n",
      "Epoch 20/500\n",
      " - 52s - loss: 0.0208 - mean_absolute_error: 0.0177 - mean_absolute_percentage_error: 1.7668 - acc: 0.9899 - categorical_accuracy: 1.0000 - val_loss: 0.0250 - val_mean_absolute_error: 0.0197 - val_mean_absolute_percentage_error: 1.9737 - val_acc: 0.9886 - val_categorical_accuracy: 1.0000\n",
      "Epoch 21/500\n",
      " - 51s - loss: 0.0208 - mean_absolute_error: 0.0178 - mean_absolute_percentage_error: 1.7821 - acc: 0.9898 - categorical_accuracy: 1.0000 - val_loss: 0.0250 - val_mean_absolute_error: 0.0199 - val_mean_absolute_percentage_error: 1.9905 - val_acc: 0.9885 - val_categorical_accuracy: 1.0000\n",
      "Epoch 22/500\n",
      " - 52s - loss: 0.0208 - mean_absolute_error: 0.0176 - mean_absolute_percentage_error: 1.7634 - acc: 0.9899 - categorical_accuracy: 1.0000 - val_loss: 0.0252 - val_mean_absolute_error: 0.0198 - val_mean_absolute_percentage_error: 1.9846 - val_acc: 0.9886 - val_categorical_accuracy: 1.0000\n",
      "Epoch 23/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-9eb618dfcdd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mstartTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mmlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_layer_perceptron\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mmetrics_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_analysis_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MLP\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Elapsed: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melapsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caducovas/DeepRinger/COC800_DataMining/cross_val_analysis.pyc\u001b[0m in \u001b[0;36mcross_val_analysis_nn\u001b[0;34m(n_split, classifier, x, y, model_name, patience, train_verbose, n_epochs)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mearlyStopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_verbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearlyStopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_verbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                 \u001b[0mtrn_desc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;31m#model = classifier.fit(x.iloc[train], y[train])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[0;32m-> 1092\u001b[0;31m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[0m\u001b[1;32m   1093\u001b[0m             raise ValueError(\n\u001b[1;32m   1094\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.pyc\u001b[0m in \u001b[0;36mis_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    733\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mx_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_dim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mx_dim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "# np.set_printoptions(precision=4)\n",
    "\n",
    "#numPos   = 63981   # Max of    63 981 samples\n",
    "#numNeg   = 63981   # Max of 1 245 005 samples\n",
    "\n",
    "numPos   = 63981   # Max of    63 981 samples\n",
    "numNeg   = 1245005   # Max of 1 245 005 samples\n",
    "\n",
    "#numPos   = 20000   # Max of    63 981 samples\n",
    "#numNeg   = 20000   # Max of 1 245 005 samples\n",
    "testSize = int((numPos+numNeg)*0.2)\n",
    "print(testSize)\n",
    "print(\"\\n\\n---- Loading and Preprocessing ----\")\n",
    "\n",
    "dataDf, labels = load_dataset(dirs.dataset, randomState=None, numPos=numPos, numNeg=numNeg)#fracPos=0.02, fracNeg=0.02)\n",
    "dataDf = preproc(dataDf, verbose=False)\n",
    "# labeledDf = dataDf.assign(Labels=labels)\n",
    "\n",
    "trainDf, testDf, y_train, y_test = train_test_split(dataDf, labels, test_size=testSize)\n",
    "\n",
    "print(\"\\nTrain data loaded with following class distributions:\")\n",
    "show_class_splits(y_train)\n",
    "print(\"\\nTest data loaded with following class distributions:\")\n",
    "show_class_splits(y_test)\n",
    "\n",
    "'Principal Components Analysis'\n",
    "'   useful to reduce dataset dimensionality'\n",
    "compactDf = dimension_reduction(dataDf, keepComp=60)\n",
    "\n",
    "print(\"\\n\\n---- Classification ----\\n\")\n",
    "\n",
    "metrics_=[]\n",
    "\n",
    "'Neural Networks'\n",
    "print(\"\\nMLP\")\n",
    "startTime = datetime.now()\n",
    "mlp = multi_layer_perceptron(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis_nn(n_split=2,classifier=mlp,x=trainDf, y=y_train,model_name=\"MLP\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(ldaPred == y_test))/testSize))\n",
    "#print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, ldaPred, normalize=True)))\n",
    "#print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, ldaPred)))\n",
    "#print(\"AUC: {:.2f}\".format(100*roc_auc_score(y_test, ldaPred)))\n",
    "#print(\"Precision: {:.2f}\".format(100*precision_score(y_test, ldaPred)))\n",
    "#print(\"Recall: {:.2f}\".format(100*recall_score(y_test, ldaPred)))\n",
    "#target_names = ['Signal', 'Background']\n",
    "#print(classification_report(y_test, ldaPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "#sns.heatmap(confusion_matrix(y_test, ldaPred),annot=True, cmap = 'YlGnBu')\n",
    "#plt.title(\"Confusion Matrix\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(995849, 51340)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf[y_train==-1].shape[0],trainDf[y_train==1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261797\n",
      "\n",
      "\n",
      "---- Loading and Preprocessing ----\n",
      "\n",
      "Original data:\n",
      "Positive examples:  63981\n",
      "Negative examples: 1245005\n",
      "\n",
      "\n",
      "Data loaded with following class distribution: \n",
      "Positive class:  0.00 %, 63981 entries \n",
      "Negative class:  0.00 %, 1245005 entries \n",
      "Total:          1308986 entries\n",
      "\n",
      "3 features containing only zeros have been dropped from data.\n",
      "\n",
      "Train data loaded with following class distributions:\n",
      "Positive class:  0.00 %, 51340 entries \n",
      "Negative class:  0.00 %, 995849 entries \n",
      "Total:          1047189 entries\n",
      "\n",
      "Test data loaded with following class distributions:\n",
      "Positive class:  0.00 %, 12641 entries \n",
      "Negative class:  0.00 %, 249156 entries \n",
      "Total:          261797 entries\n",
      "('\\nN components:', 97)\n",
      "('\\nPrincipal components to keep: ', 60)\n",
      "('\\nCompact data: ', (1308986, 60))\n",
      "\n",
      "\n",
      "---- Classification ----\n",
      "\n",
      "\n",
      "LDA\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1131e33b561a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nLDA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mstartTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mldaPred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_discriminant_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mmetrics_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"LDA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0melapsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstartTime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/caducovas/DeepRinger/COC800_DataMining/discriminant_analysis.pyc\u001b[0m in \u001b[0;36mlinear_discriminant_analysis\u001b[0;34m(x_train, y_train, x_test, y_test, n_components)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mlda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearDiscriminantAnalysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m#X_r2 = lda.fit(x_train, y_train).transform(X)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/sklearn/discriminant_analysis.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshrinkage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shrinkage not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve_svd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lsqr'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve_lsqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshrinkage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshrinkage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/sklearn/discriminant_analysis.pyc\u001b[0m in \u001b[0;36m_solve_svd\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfac\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mXc\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# SVD of centered (within)scaled data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mrank\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python2.7/site-packages/scipy/linalg/decomp_svd.pyc\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;31m# perform decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     u, s, v, info = gesXd(a1, compute_uv=compute_uv, lwork=lwork,\n\u001b[0;32m--> 129\u001b[0;31m                           full_matrices=full_matrices, overwrite_a=overwrite_a)\n\u001b[0m\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 10\n",
    "# np.set_printoptions(precision=4)\n",
    "\n",
    "#numPos   = 63981   # Max of    63 981 samples\n",
    "#numNeg   = 63981   # Max of 1 245 005 samples\n",
    "\n",
    "numPos   = 63981   # Max of    63 981 samples\n",
    "numNeg   = 1245005   # Max of 1 245 005 samples\n",
    "\n",
    "#numPos   = 20000   # Max of    63 981 samples\n",
    "#numNeg   = 20000   # Max of 1 245 005 samples\n",
    "testSize = int((numPos+numNeg)*0.2)\n",
    "print(testSize)\n",
    "print(\"\\n\\n---- Loading and Preprocessing ----\")\n",
    "\n",
    "dataDf, labels = load_dataset(dirs.dataset, randomState=None, numPos=numPos, numNeg=numNeg)#fracPos=0.02, fracNeg=0.02)\n",
    "dataDf = preproc(dataDf, verbose=False)\n",
    "# labeledDf = dataDf.assign(Labels=labels)\n",
    "\n",
    "trainDf, testDf, y_train, y_test = train_test_split(dataDf, labels, test_size=testSize)\n",
    "\n",
    "print(\"\\nTrain data loaded with following class distributions:\")\n",
    "show_class_splits(y_train)\n",
    "print(\"\\nTest data loaded with following class distributions:\")\n",
    "show_class_splits(y_test)\n",
    "\n",
    "'Principal Components Analysis'\n",
    "'   useful to reduce dataset dimensionality'\n",
    "compactDf = dimension_reduction(dataDf, keepComp=60)\n",
    "\n",
    "print(\"\\n\\n---- Classification ----\\n\")\n",
    "\n",
    "metrics_=[]\n",
    "\n",
    "'Linear Discriminant Classifier'\n",
    "print(\"\\nLDA\")\n",
    "startTime = datetime.now()\n",
    "ldaPred,lda = linear_discriminant_analysis(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=lda,x=trainDf, y=y_train,model_name=\"LDA\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(ldaPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, ldaPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, ldaPred)))\n",
    "print(\"AUC: {:.2f}\".format(100*roc_auc_score(y_test, ldaPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, ldaPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, ldaPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, ldaPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, ldaPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Quadratic Discriminant Classifier'\n",
    "print(\"\\nQDA\")\n",
    "startTime = datetime.now()\n",
    "qdaPred,qda = quadratic_discriminant_analysis(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=qda,x=trainDf, y=y_train,model_name=\"QDA\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(qdaPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, qdaPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, qdaPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, qdaPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, qdaPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, qdaPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, qdaPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, qdaPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Bayesian Classifier'\n",
    "print(\"\\nNaive Bayes\")\n",
    "startTime = datetime.now()\n",
    "bayesPred,bayes = gaussian_naive_bayes(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=bayes,x=trainDf, y=y_train,model_name=\"Bayesian Classifier\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(bayesPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, bayesPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, bayesPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, bayesPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, bayesPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, bayesPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, bayesPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, bayesPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Logistic Regression'\n",
    "print(\"\\nLogistic Regression\")\n",
    "startTime = datetime.now()\n",
    "logPred,log = log_reg(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=log,x=trainDf, y=y_train,model_name=\"Logistic Regression\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(logPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, logPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, logPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, logPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, logPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, logPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, logPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, logPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "#@@'Logistic Regression with L2 Regularization'\n",
    "#@@# TODO: Testar LogisticRegressionCV, que encontra o C otimo\n",
    "#@@logPenalty = 1/100\n",
    "\n",
    "#@@print(\"\\nLogistic Regression with L2 Regularization\")\n",
    "#@@startTime = datetime.now()\n",
    "#@@rlogPred = ridge_log_reg(trainDf, y_train, testDf, y_test, reg=logPenalty)\n",
    "#@@elapsed = datetime.now() - startTime\n",
    "#@@print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#@@print(\"Regularization paramenter (smaller is stronger): \\n\", logPenalty)\n",
    "#@@print(\"Correct predictions {:.4f}\".format(np.sum(rlogPred == y_test)/testSize))\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Linear Perceptron'\n",
    "print(\"\\nLinear Perceptron\")\n",
    "startTime = datetime.now()\n",
    "percepPred,percep = perceptron(trainDf, y_train, testDf, y_test)\n",
    "#cross_val_analysis(n_split=2,classifier=percep,trainDf=trainDf, y_train=y_train)\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(logPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, percepPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, percepPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, percepPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, percepPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, percepPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, percepPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, percepPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Nearest Neighbours'\n",
    "startTime = datetime.now()\n",
    "knnPred,knn = nearest_neighbours(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=knn,x=trainDf, y=y_train,model_name=\"Nearest Neighbours\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "print(\"\\nNearest Neighbours\")\n",
    "#print(\"Correct predictions {:.4f}\".format(np.sum(knnPred == y_test)/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, knnPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, knnPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, knnPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, knnPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, knnPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, knnPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, knnPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Decision Tree'\n",
    "print(\"\\nDecision Tree\")\n",
    "startTime = datetime.now()\n",
    "treePred,tree = decision_tree(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=tree,x=trainDf, y=y_train,model_name=\"Decision Tree\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(treePred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, treePred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, treePred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, treePred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, treePred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, treePred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, treePred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, treePred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'Random Forest'\n",
    "print(\"\\nRandom Forest\")\n",
    "startTime = datetime.now()\n",
    "forestPred,forest = random_forest(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=forest,x=trainDf, y=y_train,model_name=\"Random Forest\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(forestPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, forestPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, forestPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, forestPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, forestPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, forestPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, forestPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, forestPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\\n---------------------------\\n\")\n",
    "\n",
    "'AdaBoost'\n",
    "print(\"\\nAdaBoost\")\n",
    "startTime = datetime.now()\n",
    "adaPred,ada = ada_boost(trainDf, y_train, testDf, y_test)\n",
    "metrics_.append(cross_val_analysis(n_split=10,classifier=ada,x=trainDf, y=y_train,model_name=\"AdaBoost\"))\n",
    "elapsed = datetime.now() - startTime\n",
    "print(\"Elapsed: \"+str(elapsed)+\"s\")\n",
    "#print(\"Correct predictions {:.4f}\".format(float(np.sum(adaPred == y_test))/testSize))\n",
    "print(\"Accuracy: {:.2f}\".format(100*accuracy_score(y_test, adaPred, normalize=True)))\n",
    "print(\"F1 Score: {:.2f}\".format(100*f1_score(y_test, adaPred)))\n",
    "print(\"AUC     : {:.2f}\".format(100*roc_auc_score(y_test, adaPred)))\n",
    "print(\"Precision: {:.2f}\".format(100*precision_score(y_test, adaPred)))\n",
    "print(\"Recall: {:.2f}\".format(100*recall_score(y_test, adaPred)))\n",
    "target_names = ['Signal', 'Background']\n",
    "print(classification_report(y_test, adaPred, target_names=target_names,digits=4))\n",
    "#Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(confusion_matrix(y_test, adaPred),annot=True, cmap = 'YlGnBu')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "header = ['Model', 'Accuracy', 'Std','F1-Measure','Std','Precision','Std','Fecall','Std','ROC AUC','Std']\n",
    "keys = ['model', 'accuracy', 'accuracy_std','f1','f1_std','precision','precision_std','recall','recall_std','roc_auc','roc_auc_std']\n",
    "#sort_by_key = 'age'\n",
    "#sort_order_reverse = True\n",
    "#del(formatted_data)\n",
    "print(format_as_table(metrics_,keys,header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
